{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c8b51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# file: scrape_valorant_patch_notes.py\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE = \"https://playvalorant.com\"\n",
    "ARCHIVE_PATH = \"/{locale}/news/tags/patch-notes/\"\n",
    "LOCALE = \"en-us\"  # change to your preferred locale, e.g., \"en-gb\", \"es-es\", \"ja-jp\"\n",
    "ARCHIVE_URL = f\"{BASE}{ARCHIVE_PATH.format(locale=LOCALE)}\"\n",
    "\n",
    "OUT_JSONL = Path(\"valorant_patch_notes.jsonl\")\n",
    "OUT_CSV   = Path(\"valorant_patch_notes.csv\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; ValorantPatchScraper/1.0; +https://example.com)\"\n",
    "}\n",
    "\n",
    "def extract_patch_number(title: str) -> Optional[str]:\n",
    "    # Tries to find things like \"Patch Notes 8.08\" / \"11.04\" / \"v1.0\"\n",
    "    m = re.search(r'(\\b(?:v)?\\d+(?:\\.\\d+){0,2}\\b)', title, flags=re.IGNORECASE)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def parse_article(url: str) -> Dict:\n",
    "    \"\"\"Download and parse a single patch notes page.\"\"\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title_el = soup.find(\"h1\")\n",
    "    title = title_el.get_text(strip=True) if title_el else \"\"\n",
    "\n",
    "    # Date (often found in time tag or meta)\n",
    "    date_text = \"\"\n",
    "    time_el = soup.find(\"time\")\n",
    "    if time_el and time_el.get(\"datetime\"):\n",
    "        date_text = time_el[\"datetime\"]\n",
    "    elif time_el:\n",
    "        date_text = time_el.get_text(strip=True)\n",
    "    else:\n",
    "        # Fallback: try meta tags\n",
    "        meta_date = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "        if meta_date and meta_date.get(\"content\"):\n",
    "            date_text = meta_date[\"content\"]\n",
    "\n",
    "    # Article body: primary selector commonly used on Riot news pages\n",
    "    paragraphs = []\n",
    "    body = soup.select_one(\"div[itemprop='articleBody']\") or soup.select_one(\"article\")\n",
    "    if body:\n",
    "        for p in body.find_all([\"p\", \"li\"]):\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "    else:\n",
    "        # Last resort: all paragraphs on page\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            if text:\n",
    "                paragraphs.append(text)\n",
    "\n",
    "    content = \"\\n\".join(paragraphs).strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"published\": date_text,\n",
    "        \"patch_number\": extract_patch_number(title) or \"\",\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "async def collect_archive_links() -> List[str]:\n",
    "    \"\"\"Use Playwright to scroll the archive and collect all article links.\"\"\"\n",
    "    links = set()\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(user_agent=HEADERS[\"User-Agent\"])\n",
    "        page = await context.new_page()\n",
    "        await page.goto(ARCHIVE_URL, wait_until=\"domcontentloaded\")\n",
    "\n",
    "        last_height = 0\n",
    "        same_height_hits = 0\n",
    "\n",
    "        # Scroll until height stops increasing several times (archive fully loaded)\n",
    "        while True:\n",
    "            # Grab article card links currently in DOM\n",
    "            anchors = await page.locator(\"a:visible\").all()\n",
    "            for a in anchors:\n",
    "                try:\n",
    "                    href = await a.get_attribute(\"href\")\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    if \"/news/\" in href and href.startswith(\"/\"):\n",
    "                        links.add(BASE + href)\n",
    "                    elif href.startswith(BASE) and \"/news/\" in href:\n",
    "                        links.add(href)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Auto-scroll\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "\n",
    "            # Check if we loaded more\n",
    "            height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if height == last_height:\n",
    "                same_height_hits += 1\n",
    "            else:\n",
    "                same_height_hits = 0\n",
    "            last_height = height\n",
    "\n",
    "            # Stop after several no-growth checks\n",
    "            if same_height_hits >= 4:\n",
    "                break\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # Filter to tag pages only (defensive)\n",
    "    deduped = sorted(set(l for l in links if \"/news/\" in l))\n",
    "    return deduped\n",
    "\n",
    "def save_jsonl(rows: List[Dict], path: Path):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    print(f\"[1/3] Collecting patch links from: {ARCHIVE_URL}\")\n",
    "    all_links = asyncio.run(collect_archive_links())\n",
    "    # Keep only patch-notes pages (defensive filter)\n",
    "    patch_links = [u for u in all_links if \"/news/\" in u and \"patch-notes\" in u.lower()]\n",
    "\n",
    "    print(f\"Found {len(all_links)} news links, {len(patch_links)} look like patch notes.\")\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "\n",
    "    print(\"[2/3] Fetching and parsing patch pages…\")\n",
    "    for url in tqdm(sorted(set(patch_links))):\n",
    "        if url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "        try:\n",
    "            item = parse_article(url)\n",
    "            # Only keep if it has some content\n",
    "            if item[\"title\"] and item[\"content\"]:\n",
    "                results.append(item)\n",
    "            # Be polite\n",
    "            time.sleep(0.6)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "\n",
    "    print(f\"[3/3] Saving {len(results)} patch notes to files…\")\n",
    "    save_jsonl(results, OUT_JSONL)\n",
    "    pd.DataFrame(results).to_csv(OUT_CSV, index=False)\n",
    "\n",
    "    print(f\"Done.\\n- JSONL: {OUT_JSONL.resolve()}\\n- CSV:   {OUT_CSV.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
